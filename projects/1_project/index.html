<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Inductive biases in Transformers | Sriram Balasubramanian</title> <meta name="author" content="Sriram Balasubramanian"/> <meta name="description" content="course project"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sriramb-98.github.io/projects/1_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://sriramb-98.github.io//"><span class="font-weight-bold">Sriram</span> Balasubramanian</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/resume.pdf">resume <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Inductive biases in Transformers</h1> <p class="post-description">course project</p> </header> <article> <p><a href="/assets/pdf/image_priors_report.pdf" class="btn btn-sm z-depth-1" role="button">Link to report</a> <a href="https://github.com/SriramB-98/deep-image-priors" class="btn btn-sm z-depth-1" role="button" target="_blank" rel="noopener noreferrer">Link to code</a> <a href="https://www.youtube.com/watch?v=FmNvQclUB9Q" class="btn btn-sm z-depth-1" role="button" target="_blank" rel="noopener noreferrer">Link to video</a></p> <p>In this project, we investigate the nature of deep image priors in modern vision architectures like transformers and swin transformers by using methods introduced in “Deep Image Prior” by Ulyanov et al . This consists of using a randomly initialized neural network to perform certain reconstruction tasks like image denoising, image inpainting, super-resolution etc.</p> <p>We find the following interesting results:</p> <p>(a) Transformers have almost no inductive biases as compared to CNNs</p> <p>(b) Swin Transformers do not have any better inductive biases as compared to vanilla Transformers when it comes to image reconstruction</p> <p>(c) There are actually two kinds of inductive biases: predictive inductive biases (useful for learning an input to output function) and generative inductive biases (useful for learning a generative model), convolutional layers have both but image reconstruction tasks only measure generative biases.</p> <p>(d) Swin transformers have better predictive inductive biases but almost no generative inductive bias.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/f16_corr-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/f16_corr-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/f16_corr-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/f16_corr.png" title="Island scenario"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/f16_transformer-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/f16_transformer-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/f16_transformer-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/f16_transformer.png" title="Branch scenario"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/f16_unet-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/f16_unet-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/f16_unet-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/f16_unet.png" title="Branch scenario"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/f16_transformer-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/f16_transformer-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/f16_transformer-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/f16_transformer.png" title="Branch scenario"> </picture> </figure> </div> </div> <div class="caption"> <b>From left to right: </b> (a) Corrupted image of F16 plane (b) Image "denoised" by transformer (c) Image denoised by SkipNet (d) Image "denoised" by Swin Transformer </div> <p>We can see that Transformers do a poor job of denoising the image as compared to SkipNets. The “denoised” image output by the Transformer is still very noisy as compared to SkipNet’s denoised image. Similarly, SkipNet’s inpainting is almost flawless, with hardly any artifacts visible while the Transformer inpainting is very poor, with multiple visible artifacts. We can also see faint grid lines on the image which are remnants of the patch boundaries of the transformer. Surprisingly, Swin transformers do not perform any better than transformers at these tasks, even though they have a hierarchichal architecture which is supposed to .</p> <p>To analyze why even Swin transformers don’t work as expected, we first distinguish between two kinds of inductive biases present in models, generative biases and predictive biases. Generative bias is the bias towards generating a family of data distributions, while predictive bias is bias towards learning a particular family of input-output mapping. For example, for CNNs, shift invariance is an example of predictive bias, and not generative bias, since it is related to how the target (that is, the labels) is invariant to changes in the image. However, as we just described, convolutional layers also have generative biases, but these are very distinct from predictive biases. In general, generative inductive bias is not equivalent to or even correlated with predictive inductive bias.</p> <p>As we saw, the experiments in the deep image prior paper only test for image reconstruction tasks, which are good for measuring generative biases but not predictive biases. The hierarchical nature of the Swin transformer induces favorable predictive inductive biases but not generative inductive biases. This could be one reason why they are used in many predictive tasks like image classification, segmentation, etc but not generative tasks where CNNs are still state of the art.</p> <p>In order to test this, we can perform the following experiment. We can equip the transformer with a stack of convolutional layers, before the input of the transformer or after the output of the transformer. When equipped before the transformer, these convolutional layers induce favorable predictive inductive biases because these convolutional layers can be trained to identify input image features (like edges, corners, etc), but hardly any generative biases because they will be suppressed by the biases of the transformer which transformers the output of the convolutional layers. When equipped after the transformer, they will hardly contribute to predictive biases because the image structure in the input signal has been completely changed by the transformer, but they will induce good generative inductive biases. In some sense, this is similar to attaching a ConvNet decoder to the transformer, which inherits the favorable generative inductive biases of the decoder. According to the theory we outlined, a model with convolutional layers after the transformer output should be much better at image reconstruction as compared to a model with convolutional layers equipped before the transformer input.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/t-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/t-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/t-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/t.png" title="example image"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/3c+t-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/3c+t-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/3c+t-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/3c+t.png" title="example image"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/t+1c-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/t+1c-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/t+1c-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/t+1c.png" title="example image"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/t+3c-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/t+3c-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/t+3c-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/t+3c.png" title="example image"> </picture> </figure> </div> </div> <div class="caption"> PSNR vs iteration for different configurations of convolutional layers and transformer. <br> <b>From left to right: </b> (a) Plain transformer (b) 3 convolutional layers before a transformer (c) 1 convolutional layer after a transformer (d) 3 convolutional layers after a transformer </div> <p>As we can observe, even adding 3 convolutional layers before the transformer does not really help, since the generative bias due to the convolutions is negligible. However, even adding one convolutional layer after the transformer really helps the gap between the PSNRs . Even after 4000 iterations, the gap between the PSNRs is still positive for the transformer with 1 convolutional layer, with higher max PSNR. Adding 3 conv layers after the transformer only helps more, as the gap is still high and positive even after 5000 iterations.</p> <p>More details are there in the report and the the code</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sriram Balasubramanian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>