---
---

@inproceedings{balasubramanian-etal-2020-whats,
	equal_authors=3,
	 bibtex_show={true},
    title = "What{'}s in a Name? Are {BERT} Named Entity Representations just as Good for any other Name?",
    author = "Sriram Balasubramanian* and
      Naman Jain* and
      Gaurav Jindal* and
      Abhijeet Awasthi and
      Sunita Sarawagi",
    booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.repl4nlp-1.24",
    doi = "10.18653/v1/2020.repl4nlp-1.24",
    pages = "205--214",
    abstract = "We evaluate named entity representations of BERT-based NLP models by investigating their robustness to replacements from the same typed class in the input. We highlight that on several tasks while such perturbations are natural, state of the art trained models are surprisingly brittle. The brittleness continues even with the recent entity-aware BERT models. We also try to discern the cause of this non-robustness, considering factors such as tokenization and frequency of occurrence. Then we provide a simple method that ensembles predictions from multiple replacements while jointly modeling the uncertainty of type annotations and label predictions. Experiments on three NLP tasks shows that our method enhances robustness and increases accuracy on both natural and adversarial datasets.",
}

@misc{balasubramanian2022better,
 bibtex_show={true},
      title={Towards Better Input Masking for Convolutional Neural Networks}, 
      author={Sriram Balasubramanian and Soheil Feizi},
      year={2022},
      eprint={2211.14646},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url = "https://arxiv.org/abs/2211.14646",
      abstract = "The ability to remove features from the input of machine learning models is very important to understand and interpret model predictions. However, this is non-trivial for vision models since masking out parts of the input image and replacing them with a baseline color like black or grey typically causes large distribution shifts. Masking may even make the model focus on the masking patterns for its prediction rather than the unmasked portions of the image. In recent work, it has been shown that vision transformers are less affected by such issues as one can simply drop the tokens corresponding to the masked image portions. They are thus more easily interpretable using techniques like LIME which rely on input perturbation. Using the same intuition, we devise a masking technique for CNNs called layer masking, which simulates running the CNN on only the unmasked input. We find that our method is (i) much less disruptive to the model's output and its intermediate activations, and (ii) much better than commonly used masking techniques for input perturbation based interpretability techniques like LIME. Thus, layer masking is able to close the interpretability gap between CNNs and transformers, and even make CNNs more interpretable in many cases."
}

@misc{anshumaan2022simulating,
	 bibtex_show={true},
      title={Simulating Network Paths with Recurrent Buffering Units}, 
      author={Divyam Anshumaan* and Sriram Balasubramanian* and Shubham Tiwari and Nagarajan Natarajan and Sundararajan Sellamanickam and Venkata N. Padmanabhan},
      url = "https://arxiv.org/abs/2202.13870",
      year={2022},
      arxiv={2202.13870},
      archivePrefix={arXiv},
      primaryClass={cs.NI},
      abstract = "Simulating physical network paths (e.g., Internet) is a cornerstone research problem in the emerging sub-field of AI-for-networking. We seek a model that generates end-to-end packet delay values in response to the time-varying load offered by a sender, which is typically a function of the previously output delays. We formulate an ML problem at the intersection of dynamical systems, sequential decision making, and time-series generative modeling. We propose a novel grey-box approach to network simulation that embeds the semantics of physical network path in a new RNN-style architecture called Recurrent Buffering Unit, providing the interpretability of standard network simulator tools, the power of neural models, the efficiency of SGD-based techniques for learning, and yielding promising results on synthetic and real-world network traces.",
      note="* indicates equal contribution"
}




@misc{sadasivan2023aigenerated,
	 bibtex_show={true},
      title={Can AI-Generated Text be Reliably Detected?}, 
      author={Vinu Sankar Sadasivan and Aounon Kumar and Sriram Balasubramanian and Wenxiao Wang and Soheil Feizi},
      year={2023},
      eprint={2303.11156},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url = "https://arxiv.org/abs/2303.11156",
      abstract = "The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, both empirically and theoretically, we show that these detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of the generative text model, can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors and zero-shot classifiers. We then provide a theoretical impossibility result indicating that for a sufficiently good language model, even the best-possible detector can only perform marginally better than a random classifier. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden watermarking signatures and add them to their generated text to be detected as text generated by the LLMs, potentially causing reputational damages to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text."
}
